{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6844c969",
   "metadata": {},
   "source": [
    "# Identifying domains with responsible editor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea247132",
   "metadata": {},
   "source": [
    "The objective of this notebook is to automatically identify web archive material that falls in access category 2, meaning domains that on their front page have decleared responsible editorship.\n",
    "\n",
    "Identification is rule-based, and based on two different forms of declarations:\n",
    "- in text (variants of \"Ansvarlig redaktør\")\n",
    "- in links (variants of \"nored.no/redaktoeransvar\" or \"presse.no/Etisk-regelverk/Redaktoerplakaten\"\n",
    "\n",
    "Based on analysis with regular expressions (regex), the script calculates a simple confidence score:\n",
    "- if regex is found on the front page for link, it appends a score of 0.5\n",
    "- if the regex matches text on the front page, the domain is appended a score of 0.5\n",
    "    - if regex does not match text on the front page, it text a regex pattern with with less authoritative terms that, combined with a link match, still is considered probable\n",
    "    - if alternative text pattern is found, the domain is appended a score of 0.25\n",
    "- if regex is not found for neither text nor link, the confidence score is 0.0\n",
    "\n",
    "From testing, we have not experienced any false positives. However, the script still produces some false negatives. The common reason for this is the these pages desclare this in dynamic content (.js) which is not handled yet by this script.\n",
    "\n",
    "Domains with a confidence score below 0.75 should therefore - until further - be checked manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ccc2c",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df76c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import warcio\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "import re\n",
    "import glob\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4ac5c0",
   "metadata": {},
   "source": [
    "## Extract html front pages from WARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4223988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for filtering front pages\n",
    "def is_front_page(url):\n",
    "    \"\"\"\n",
    "    Determines if URL is front page, based on path structure.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    if parsed_url.path in ['', '/']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function for excluding \"front pages\" of certain subdomains\n",
    "def exclude_subdomain(url, excluded_subdomains):\n",
    "    \"\"\"\n",
    "    Checks if URL's subdomain is in the list of excluded subdomains.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    hostname = parsed_url.hostname\n",
    "    # Handle cases with 'www.' as subdomain\n",
    "    subdomains = hostname.replace('www.', '').split('.')[:-2]\n",
    "    for subdomain in subdomains:\n",
    "        if subdomain in excluded_subdomains:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Subdomains to exclude\n",
    "excluded_subdomains = ['samtykke', 'samtykker', 'stilling', 'stillinger', 'personvern']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ecfd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for WARC source and HTML extraction\n",
    "warc_path = '../data/veidemann/filename.warc.gz'\n",
    "html_folder = 'htmls/veidemann/'\n",
    "os.makedirs(html_folder, exist_ok=True)\n",
    "\n",
    "with gzip.open(warc_path, 'rb') as stream:\n",
    "    for record in ArchiveIterator(stream):\n",
    "        content_type = record.http_headers.get_header('Content-Type') if record.http_headers else None\n",
    "        status_code = record.http_headers.get_statuscode() if record.http_headers else None\n",
    "        if record.rec_type == 'response' and content_type and 'text/html' in content_type and status_code.startswith('2'):\n",
    "            url = record.rec_headers.get_header('WARC-Target-URI')\n",
    "            # Check if URL is front page or not\n",
    "            if is_front_page(url) and not exclude_subdomain(url, excluded_subdomains):\n",
    "                html_content = record.content_stream().read()\n",
    "                # Generate unique filename for each html file with reference to WARC record timestamp and (sub)domain\n",
    "                filename = f\"{record.rec_headers.get_header('WARC-Date')}_{url.split('//')[1].split('/')[0]}.html\"\n",
    "                with open(os.path.join(html_folder, filename), 'wb') as f:\n",
    "                    f.write(html_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ef78a1",
   "metadata": {},
   "source": [
    "## Indentify regex in text and link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539d7304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex pattern for textual declaration of responsible editorship\n",
    "text_pattern = r\"Ansvarlig redaktør|Ansvarleg redaktør|Ansv\\.?\\s*redaktør|Ansv\\.?\\s*red\\.|Ansvarlig red\\.|Ansvarleg red\\.|Sjefsred\\.?|Sjefsredaktør|Sjefred\\.?|Sjefredaktør|Ansvarlig redakt&oslash;r|Ansvarleg redakt&oslash;r|Ansv\\.?\\s*redakt&oslash;r|Sjefsredakt&oslash;r|Sjefredakt&oslash;r|Váldodoaimmaheaddji\"\n",
    "\n",
    "# Regex pattern for alternative textual declaration if no match for 'text_pattern'\n",
    "text_pattern_alternative = r\"Redaktør|Konstituert redaktør\"\n",
    "\n",
    "# Regex pattern for links declaring responsible editorship\n",
    "url_pattern = r\"https?://(www\\.)?nored\\.no/Redaktoeransvar(/Redaktoerplakaten)?/?|https?://(www\\.)?nored\\.no/Redaktoerplakaten(/Redaktoerplakaten)?/?|https?://(www\\.)?presse\\.no/pfu/etiske-regler(/Redaktoerplakaten)?/?|https?://(www\\.)?presse\\.no/Etisk-regelverk(/Redaktoerplakaten)?/?|https?://presse\\.no/etisk-regelverk/vaer-varsom-plakaten|http://presse.no/pfu/etiske-regler/vaer-varsom-plakaten/|https?://(www\\.)?presse\\.no/pfu/etiske-regler/(redaktorplakaten)/?|http://presse\\.no/etisk-regelverk/vaer-varsom-plakaten|https?://(no\\.)?wikipedia\\.org/wiki/V%C3%A6r_Varsom-plakaten\"\n",
    "\n",
    "# List for results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840aa5d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loop to iterate over html files\n",
    "for html_file in glob.glob(os.path.join('htmls/veidemann/newWarc/', '*.html')):\n",
    "    print(\"Processing file:\", html_file)\n",
    "    \n",
    "    try:\n",
    "        with open(html_file, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            with open(html_file, 'r', encoding='latin-1') as file:\n",
    "                html_content = file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Could not read file {html_file} due to encoding issues.\")\n",
    "            continue  # Skip to next file\n",
    "\n",
    "    # Extract URL from file name\n",
    "    file_name = html_file.split('/')[-1]\n",
    "    url = 'http://' + file_name.split('_')[1].split('/')[0]\n",
    "\n",
    "    # Parse URL and extract host\n",
    "    parsed_url = urlparse(url)\n",
    "    host = parsed_url.netloc\n",
    "\n",
    "    # Check text and link\n",
    "    text_found = bool(re.search(text_pattern, html_content, re.IGNORECASE))\n",
    "    alt_text_found = False\n",
    "\n",
    "    # If no match, check the alternative pattern\n",
    "    if not text_found:\n",
    "        alt_text_found = bool(re.search(text_pattern_alternative, html_content, re.IGNORECASE))\n",
    "\n",
    "    link_found = bool(re.search(url_pattern, html_content, re.IGNORECASE))\n",
    "\n",
    "    # Append to results\n",
    "    results.append((host, text_found, alt_text_found, link_found))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd8ae3",
   "metadata": {},
   "source": [
    "## Convert results to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40010cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all lines\n",
    "pd.set_option('display.max_rows', None)\n",
    "df = pd.DataFrame(results, columns=['Host', 'Text_Found', 'Alt_Text_Found', 'Link_Found'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064fa69c",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a167311",
   "metadata": {},
   "source": [
    "##### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8af206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean values in 'Host' column for port and file extension\n",
    "df['Cleaned_Host'] = df['Host'].str.replace(r\"(:\\d+)?\\.html\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932669c3",
   "metadata": {},
   "source": [
    "##### Aggregate results and calculate confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df830d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of files which matches either text, link or both.\n",
    "total_files = len(df)\n",
    "text_percentage = (df['Text_Found'].sum() / total_files) * 100\n",
    "alt_text_percentage = (df['Alt_Text_Found'].sum() / total_files) * 100\n",
    "link_percentage = (df['Link_Found'].sum() / total_files) * 100\n",
    "both_percentage = ((df['Text_Found'] & df['Link_Found']).sum() / total_files) * 100\n",
    "\n",
    "# Calculate confidence score\n",
    "def assign_confidence_score(row):\n",
    "    score = 0\n",
    "    if row['Text_Found']:\n",
    "        score += 0.5\n",
    "    if 'Alt_Text_Found' in row and row['Alt_Text_Found']:\n",
    "        score += 0.25\n",
    "    if row['Link_Found']:\n",
    "        score += 0.5\n",
    "    return score\n",
    "\n",
    "\n",
    "df['Confidence_Score'] = df.apply(assign_confidence_score, axis=1)\n",
    "\n",
    "# Group by cleaned host and calculate confidence score for each host (rounded to 2 decimals)\n",
    "grouped = df.groupby('Cleaned_Host')['Confidence_Score'].mean().reset_index()\n",
    "grouped['Confidence_Score'] = grouped['Confidence_Score'].round(2)\n",
    "\n",
    "# Sort host by 'Confidence_Score' (descending)\n",
    "grouped_sorted = grouped.sort_values(by='Confidence_Score', ascending=False)\n",
    "\n",
    "# Print percentages\n",
    "print(f\"Tekst funnet totalt: {text_percentage:.2f}%\")\n",
    "print(f\"Alt. tekst funnet totalt: {alt_text_percentage:.2f}%\")\n",
    "print(f\"Link funnet totalt: {link_percentage:.2f}%\")\n",
    "print(f\"Tekst OG lenke funnet totalt: {both_percentage:.2f}%\")\n",
    "\n",
    "# Print hosts by confidence score\n",
    "print(grouped_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279abc3d",
   "metadata": {},
   "source": [
    "## Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd530a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for export\n",
    "# excel_path = './report/frontpages_grouped_sorted.xlsx'\n",
    "# grouped_sorted.to_excel(excel_path, index=False, engine='openpyxl')\n",
    "\n",
    "# print(f\"Data exported to {excel_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c69bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
